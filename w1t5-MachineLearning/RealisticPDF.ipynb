{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0c38164",
   "metadata": {},
   "source": [
    "# Real life PDF fit\n",
    "\n",
    "In the previous tutorial (PDF Fitting) we have fitted a Neural Network to PDF data that we have obtained from LHAPDF. Ah! If life were so simple!\n",
    "\n",
    "Reality is much more complicated:\n",
    "\n",
    "1. We cannot measure the PDF: we have no PDF data!\n",
    "\n",
    "2. The data has some uncertainties associated to it.\n",
    "\n",
    "In this tutorial we are going to do a more realistic (albeit simplified) PDF fit.\n",
    "\n",
    "## From the PDF to the experimental data\n",
    "\n",
    "In an experiment we only have access to observables. These observables, while they can be computed theoretically, depend on the PDF in a non trivial manner, for hadronic collision (such as those at the LHC) we have:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{O} = \\displaystyle\\sum_{ij} \\int dx_{1} dx_{2}  \\  f_{i}  (x_1, \\mu_F) \\  f_{j}(x_2, \\mu_F) \\ \\hat{\\sigma}_{ij}(x_{1}, x_{2}, \\mu_{R}, \\mu_{F})\n",
    "\\end{equation}\n",
    "\n",
    "For simplicity (and because that topic was already covered in the tutorials of the third day) we are going to drop the dependence on $\\mu_F$ from the PDF. All scale-dependence is contained in the partonic cross section instead\n",
    "\n",
    "Utilizing the model of the PDF that we built in the previous tutorial our PDF have instead the following form: $f_{i}(x) = (1 - x)^{1+\\beta}NN_{i}(x)$ where the index $i$ refers to the parton.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{O} = \\displaystyle\\sum_{ij} \\int dx_{1} dx_{2}  \\ (1 - x_1)^{1+\\beta}NN_{i}(x_1) \\ (1 - x_2)^{1+\\beta}NN_{j}(x_2) \\ \\hat{\\sigma}_{ij}(x_{1}, x_{2}, \\mu_{R}, \\mu_{F})\n",
    "\\end{equation}\n",
    "\n",
    "Note that in the previous equation both Neural Networks are the same, but are evaluated at different values of $x$ (and potentially contribute with different partons $i$ and $j$). This means the observable depends non-linearly on the Neural Network, which greatly complicates the training. For this tutorial we are going to limit ourselves to DIS observables so one of the two PDFs is set to 1, which will facilitate the construction of the network, but in a global PDF fit both DIS and double-hadronic observables need to be considered.\n",
    "\n",
    "Up to this point we have used Mean Squared Errors as the loss function to be optimized. In this case we are comparing datapoints ($D$) with observables ($\\mathcal{O}$). The loss function thus looks like:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{N}\\sum_{k} (\\mathcal{O}_{k} - D_{k})^2 = \\frac{1}{N}\\sum_{k}\\left(\\displaystyle\\sum_{ij} \\int dx_{1} dx_{2}  \\ (1 - x_1)^{1+\\beta}NN_{i}(x_1) \\ (1 - x_2)^{1+\\beta}NN_{j}(x_2) \\ \\hat{\\sigma}_{ij}^{(k)}(x_{1}, x_{2}, \\mu_{R}, \\mu_{F}) - D_{k}\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "with $k$ running over datapoints in the fit. This loss function corresponds and it is usually called $\\chi^{2}$.\n",
    "\n",
    "\n",
    "## Experimental uncertainties\n",
    "\n",
    "In addition to the previous consideration, we need to include the information about the experimental uncertainties. Experimental uncertainties introduce correlations between datapoints which need to be taken into account:\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\frac{1}{N}\\sum_{k,l} (\\mathcal{O}_{k} - D_{k})s_{kl}^{-1}(\\mathcal{O}_{l} - D_{l})\n",
    "\\end{equation}\n",
    "\n",
    "With $s_{kl}^{-1}$ the inverse of the covariance matrix. In the limit of a diagional covariance matrix (no correlation between datapoints) one would recover the simpler form that we have used before.\n",
    "\n",
    "## Tutorial outline\n",
    "\n",
    "In this tutorial we are going to take the final multi-flavour PDF we constructed in the previous one as the starting point.\n",
    "\n",
    "In order to have a realistic-looking PDF from only a few datasets, we are going to use the `.npz` files that you downloaded in the first day of the school. They contain:\n",
    "\n",
    "- `D`: the experimental data\n",
    "- `covmat`: the experimetal covariance matrix\n",
    "- `fktable`: an interpolation table for the partonic cross section the fktable is a tensor `(ndata, luminosity channel, x)`\n",
    "- `xgrid`: grid in x in which to evaluate the PDF\n",
    "- `luminosity`: the relevant indices of the luminosity\n",
    "\n",
    "The first thing we will do is to create an observable that we can compare to data. In a first step we will simply compare to data without taking into account the experimental uncertainties. \n",
    "\n",
    "Then we will create a custom loss function, introducing the covariance matrix into the problem.\n",
    "\n",
    "We will finish the tutorials creating replicas of the data to generate a PDF ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea65cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from lhapdf import setVerbosity\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "setVerbosity(0)\n",
    "\n",
    "data_folder = Path(\"data\") / \"pdf_fit\"\n",
    "if not data_folder.exists():\n",
    "    print(\"Warning! The data folder does not exist!\")\n",
    "\n",
    "available_datasets = [\"HERACOMBNCEP920\", \"HERACOMBCCEM\", \"SLACP\", \"HERACOMB_SIGMARED_B\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6f860",
   "metadata": {},
   "source": [
    "## 1. Prepare the PDF model\n",
    "\n",
    "1. Prepare a PDF model that takes as input `x` and outputs `9` different flavours. You should be able to use what you wrote in the previous tutorial\n",
    "\n",
    "2. Create a layer that rotate a PDF to the evolution basis, in which the fk-tables are generated (as you learn two tutorials ago!)\n",
    "\n",
    "\n",
    "code suggestions:\n",
    "```python\n",
    "\n",
    "# Model building\n",
    "class Preprocessing(tf.keras.layers.Layer):\n",
    "    \"\"\"This layer generates a preprocessing (1-x)**(1+beta)\"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"The build function will be called before a forward pass and the trainable weight\n",
    "        will be generated. Beta is constrained to be a positive value to avoid 1/0\"\"\"\n",
    "        self._beta = self.add_weight(\n",
    "            shape=(1,),\n",
    "            trainable=True,\n",
    "            name=\"beta\",\n",
    "            constraint=tf.keras.constraints.non_neg(),\n",
    "            initializer=\"ones\",\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return (1.0 - x) ** (self._beta + 1.0)\n",
    "\n",
    "\n",
    "class InputScaling(tf.keras.layers.Layer):\n",
    "    \"\"\"This layer applies a logarithmic scaling to the input and then concatenates it to the actual input\n",
    "    This layer is dim=1 --> dim=2\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.concat([x, tf.math.log(x)], axis=-1)\n",
    "\n",
    "\n",
    "def generate_pdf_model(outputs=9, units=16, nlayers=4, activation=\"tanh\"):\n",
    "    \"\"\"Generate a PDF model such that\n",
    "    f(x) = (1-x)^beta * NN(x, log(x))\n",
    "    \"\"\"\n",
    "    # Note that we have added a \"None\" size here, we will see in a moment why!\n",
    "    input_layer = tf.keras.layers.Input(shape=(None, 1))\n",
    "    scaled_input = InputScaling()\n",
    "    preprocessing_factor = Preprocessing()\n",
    "    mm_layer = tf.keras.layers.Multiply()\n",
    "\n",
    "    # Prepare the sequential PDF model\n",
    "    pdf_raw = Sequential(name=\"pdf\")\n",
    "    pdf_raw.add(scaled_input)\n",
    "    for _ in range(nlayers):\n",
    "        pdf_raw.add(keras.layers.Dense(units, activation=activation))\n",
    "    pdf_raw.add(keras.layers.Dense(outputs, activation=\"linear\"))\n",
    "\n",
    "    final_result = mm_layer([pdf_raw(input_layer), preprocessing_factor(input_layer)])\n",
    "    return tf.keras.models.Model(input_layer, final_result)\n",
    "\n",
    "\n",
    "pdf_model = generate_pdf_model()\n",
    "test = pdf_model(np.random.rand(2, 20, 1))\n",
    "pdf_model.summary()\n",
    "\n",
    "# Rotation to the evolution basis\n",
    "class EvolutionRotation(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    While the PDFs that we are fitting are in the basis of flavours. Due to the peculiarities of the DGLAP evolution, which is contained in the fktable together with the partonic cross section, it is more convenient to perform the fit in what is known as the \"evolution basis\". And the fktables expect the PDFs to be, indeed in the evolution basis.\n",
    "In this tutorial we are going to keep the fit in the flavour basis but this means we need a rotation from the NN output into the evolution basis.\n",
    "Note that the PDFs we convolute are fitted at a scale of  ð‘„=1.65  and contain no contribution of the bottom or top quark. In addition we are not consider a photon contribution in this tutorial.\n",
    "\n",
    "    This layer takes a single PDF = NN(x)*(1-x)^beta in the flavour basis and rotates\n",
    "    to the evolution basis\n",
    "\n",
    "                                                       0   1   2   3   4  5  6  7  8\n",
    "    The flavour basis of the NN model in our case is (g, -c, -s, -u, -d, d, u, s, c)\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, pdf):\n",
    "        # The input pdf has shape (batch, nx, flavours)\n",
    "        pdfT = tf.transpose(pdf)\n",
    "\n",
    "        singlet = tf.reduce_sum(pdfT, axis=0)\n",
    "        g  = pdfT[0]\n",
    "        v = (pdfT[6]-pdfT[3]) + (pdfT[5]-pdfT[4]) + (pdfT[7]-pdfT[2]) + (pdfT[8]-pdfT[1])\n",
    "        v3  = (pdfT[6]-pdfT[3]) - (pdfT[5]-pdfT[4]) \n",
    "        v8  = (pdfT[6]-pdfT[3]) + (pdfT[5]-pdfT[4]) - 2*(pdfT[7]-pdfT[2])\n",
    "        v15 = (pdfT[6]-pdfT[3]) + (pdfT[5]-pdfT[4]) + (pdfT[7]-pdfT[2]) - 3*(pdfT[8]-pdfT[1])\n",
    "        t3  = (pdfT[6]+pdfT[3]) - (pdfT[5]+pdfT[4])\n",
    "        t8  = (pdfT[6]+pdfT[3]) + (pdfT[5]+pdfT[4]) - 2*(pdfT[7]+pdfT[2])\n",
    "        t15 = (pdfT[6]+pdfT[3]) + (pdfT[5]+pdfT[4]) + (pdfT[7]+pdfT[2]) - 3*(pdfT[8]+pdfT[1])\n",
    "\n",
    "        # All other members of the evolution basis contain redundant information at the fitting scale\n",
    "        photon = tf.zeros_like(g)\n",
    "        v24 = v\n",
    "        v35 = v\n",
    "        t24 = singlet\n",
    "        t35 = singlet\n",
    "\n",
    "        pdf_evol_list = [photon, singlet, g, v, v3, v8, v15, v24, v35, t3, t8, t15, t24, t35]\n",
    "        # The output will be (nx, 14)\n",
    "        return tf.concat(pdf_evol_list, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2270f2",
   "metadata": {},
   "source": [
    "### 2. Create a trainable model for which the output is the data!\n",
    "\n",
    "In order to use the built-in algorithms in tensorflow for training, we need to write the integral over the `x` in a way that tensorflow can understand.\n",
    "Furthermore, computing the complete integral in a per-epoch or per-event basis would be too computationally expensive.\n",
    "Thanks to the power of the FK-Tables we can make that integral into a convolution, also with TensorFlow!\n",
    "\n",
    "Furthermore, the raining on datasets is a bit more subtle than the simple training with the PDF as the target.\n",
    "\n",
    "When training against a PDF we had a situation in which every point in the input corresponds to a single point in the output, so the loss function (and the model) is a relatively simple one:\n",
    "\n",
    "\\begin{equation}\n",
    "    l(x) = y(x) - t(x)\n",
    "\\end{equation}\n",
    "\n",
    "However, now in order to train against data we need to perform the integral (which we approximate by a convolution). This means that many values of `x` correspond to a single value of the output. And many values of the output are generated by the same input grid.\n",
    "\n",
    "\n",
    "1. Write a \"Convolution\" layer that is constructed taking as input the fktable and the basis of flavours that the fktables uses and computes the predictions to be compared with the experimental data.\n",
    "\n",
    "2. Write a function to compare the results of your fit with the actual experimental data and look at the comparison.\n",
    "\n",
    "3. Write a loss function so that you can train a model which includes a convolution\n",
    "\n",
    "4. Fit against one of the dataset, you choose which.\n",
    "\n",
    "5. Compare the results of your fit with your dataset.\n",
    "\n",
    "6. Compare now the results of your fit for a different dataset! What has happened? \n",
    "\n",
    "\n",
    "code suggestion:\n",
    "```python\n",
    "\n",
    "# Wrapper function to compare the predictions of a PDF with the results of a dataset\n",
    "def theory_data_comparison(dsname, pdf):\n",
    "    \"\"\"Compare the predictions of a given pdf with the selected dataset\n",
    "    \n",
    "    Note that in this example we are using the custom layers that we have created as if they were\n",
    "    numpy operations, without creating a model out of them!\n",
    "    \n",
    "    This function assumes that there are two layers, change it accordingly if it is not the case for you\n",
    "        - EvolutionRotation\n",
    "        - Convolution\n",
    "    \"\"\"\n",
    "    # The data is in the `.npz` format which can be loaded by the numpy \"load\" function\n",
    "    data = np.load(data_folder / f\"{dsname}.npz\")\n",
    "    x = data.get(\"xgrid\").reshape(1, -1, 1)\n",
    "    lbasis = data.get(\"luminosity\")\n",
    "    fktable = data.get(\"fktable\")\n",
    "    experimental_data = data.get(\"D\")\n",
    "\n",
    "    # Rotate the PDF (evaluated in the grid in x)\n",
    "    evolution_pdf = EvolutionRotation()(pdf(x))\n",
    "    convolution_layer = Convolution(fktable, lbasis)\n",
    "    \n",
    "    theory_predictions = convolution_layer(evolution_pdf)\n",
    "    \n",
    "    # Approximate experimental errors with the diagonal of the covariance matrix\n",
    "    errors = np.sqrt(np.diag(data.get(\"covmat\")))\n",
    "    idata = np.arange(len(experimental_data)) # we don't have information about the kinematic variable\n",
    "    \n",
    "    plt.title(f\"Theory-data comparison for {dsname}\")\n",
    "        \n",
    "    plt.errorbar(idata, experimental_data, yerr=errors, fmt=\".\", label=\"Experimental data\")\n",
    "    plt.errorbar(idata, theory_predictions, yerr=0.0, fmt=\"x\", color=\"red\", label=\"Prediction\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Data index\")\n",
    "    \n",
    "# Construct loss function that is able to digest both the output of the model and the experimental data \n",
    "def chi2_simplified(ytrue, ypred):\n",
    "    \"\"\"Loss function to pass to the model\"\"\"\n",
    "    return tf.reduce_sum((ytrue - ypred) ** 2)\n",
    "    \n",
    "# In order to train against data, you can do the following:   \n",
    "dsname = available_datasets[3]\n",
    "data = np.load(data_folder / f\"{dsname}.npz\")\n",
    "# We modify the input so that it takes 3 axis (even if two of them size=1)\n",
    "x = data.get(\"xgrid\").reshape(1, -1, 1)  # 1 batch, N datapoints, n dim\n",
    "lbasis = data.get(\"luminosity\")\n",
    "fktable = data.get(\"fktable\")\n",
    "\n",
    "# We would need to also reshape the experimental data, so that we match through the batch dimension\n",
    "experimental_data = data.get(\"D\").reshape(1, -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62245d3",
   "metadata": {},
   "source": [
    "### 3. Train on multiple dataset and with experimental errors\n",
    "\n",
    "For that we are going to generate several separated observable models that all use the same pdf models.\n",
    "At the end we will concatenate all such models. At the end we will compare with the same data for HERACOMB_SIGMARED_B.\n",
    "\n",
    "Note that the input grid in x is always the same. This allow us to greatly simplify the model. In general they can be different and this could be treated from the point of view of your model (where each observable will take a different input) or by a preprocessing of the data (adding extra 0s if necessary).\n",
    "\n",
    "1. Train all datasets at the same time. There are two possibilities for this, either concatenating all outputs, or creating a loss function per dataset. Note that, since in either case the PDF model will be a single one, you will always be training the same PDF!\n",
    "2. Check your results.\n",
    "3. (optional) train leaving one of the datasets out, to see how the model generalizes\n",
    "\n",
    "code suggestions:\n",
    "\n",
    "```python\n",
    "\n",
    "# Create an observable per dataset\n",
    "observables = [ ]\n",
    "\n",
    "## If you choose to concatenate\n",
    "# Use the Keras `Concatenate` layer, to create a concatenation of observables\n",
    "final_layer = keras.layers.Concatenate()(observables)\n",
    "# Use said concatenation as the model output, now your output will be a concatenation of all data\n",
    "final_model = keras.models.Model(model_input, final_layer)\n",
    "\n",
    "## If you choose to use a different loss per output\n",
    "chi2_list = []\n",
    "for covmat in covmats:\n",
    "    chi2_list.append(chi2_function)\n",
    "final_model = keras.models.Model(model_input, observables)\n",
    "final_model.compile(keras.optimizers.Nadam(), loss=chi2_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a6c14",
   "metadata": {},
   "source": [
    "### 4. Include experimental errors\n",
    "\n",
    "Let us summarize what we have done up to now: we have create one single PDF model which takes as input a single grid in x and produces a pdf values for this grid in x. Then the results of this PDF are rotated into the evolution basis and convoluted with an interpolation table to produce a physical observable.\n",
    "\n",
    "Note that while all our models utilize the same convolution and rotation, this is not a requirement. Indeed, we could in the same fit include FKTables for DIS and hadronic observables, include extra contribution or physical constraints.\n",
    "\n",
    "For the output of the model, we have considered that all datapoints are created equal and have concatenated the outputs and compared against a concatenation of the experimental results. This is, as well, not a requirement.\n",
    "In the next exercise we need to use a different loss per output (i.e., one per dataset) since each loss will be different due to the covariance matrix!\n",
    "\n",
    "1. Write class such that you can generate a different loss function per experiment with different data\n",
    "2. Modify the function call so that the covariance matrix is taken into account during the computation of the loss\n",
    "3. Modify the script to compare model and data so that PDF errors are taken into account.\n",
    "4. Fit and check!\n",
    "\n",
    "\n",
    "code suggestion:\n",
    "```python\n",
    "\n",
    "class Chi2:\n",
    "    def __init__(self, covmat):\n",
    "        self._invcovmat = tf.constant(np.linalg.inv(covmat), dtype=tf.float32)\n",
    "        \n",
    "    def __call__(self, ytrue, ypred):\n",
    "        tmp = (ytrue - ypred)\n",
    "        return tf.einsum(\"bi,ij,bj->b\", tmp, self._invcovmat, tmp)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf3c34",
   "metadata": {},
   "source": [
    "### 5. Generate PDF errors\n",
    "Finally, we are going to use the Monte Carlo replica method to generate also an error bar for the predictions.\n",
    "\n",
    "The basis of this method, once we have arrived to this point, is actually quite simple. We are going to generate variations of the output data according to the covariance matrix of each dataset. These variations will be random.\n",
    "\n",
    "Then you will train separate PDF models so that each one of them is optimized with a different \"replica\". At the end you can use this set of separate PDF models to generate uncertainties.\n",
    "\n",
    "1. Generate variations of the datasets. Try to do it on your own! Otherwise there's a suggestion below\n",
    "2. Train a number of independent PDF models (i.e., 5)\n",
    "3. Modify the comparison wrapper so that it can also plot uncertainties for the predictions of the model\n",
    "\n",
    "\n",
    "code suggestion\n",
    "\n",
    "```python\n",
    "# let's save all the information that is shared by all the replicas\n",
    "# (note that the PDF model is not among that!)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    name: str\n",
    "    expdata: np.ndarray\n",
    "    covmat: np.ndarray\n",
    "    convolution: Convolution\n",
    "    chi2: Chi2\n",
    "\n",
    "    @property\n",
    "    def ndata(self):\n",
    "        return self.expdata.shape[-1]\n",
    "\n",
    "    def generate_replica(self):\n",
    "        r = -0.5 + np.random.rand(self.ndata)\n",
    "        return self.expdata + np.dot(self.covmat, r).reshape(1, -1)\n",
    "\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for dsname in available_datasets:\n",
    "    data = np.load(data_folder / f\"{dsname}.npz\")\n",
    "    x = data.get(\"xgrid\").reshape(1, -1, 1)\n",
    "    lbasis = data.get(\"luminosity\")\n",
    "    fktable = data.get(\"fktable\")\n",
    "    covmat = data.get(\"covmat\")\n",
    "    edata = data.get(\"D\").reshape(1, -1)\n",
    "\n",
    "    chi2 = Chi2(covmat)\n",
    "    cc = Convolution(fktable, lbasis)\n",
    "    dd = Dataset(dsname, edata, covmat, cc, chi2)\n",
    "\n",
    "    datasets.append(dd)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
